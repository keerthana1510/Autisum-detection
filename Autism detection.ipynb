{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26544,"status":"ok","timestamp":1712942697327,"user":{"displayName":"Akanksha Vennu","userId":"16862492790728247499"},"user_tz":-330},"id":"Ql6uqlowGCic","outputId":"edb7cad6-b319-46c9-ff14-0a5f58963efd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","# Preprocessing parameters\n","IMG_SIZE = 224\n","BATCH_SIZE = 32\n","batch_size=32\n","# Define data directories\n","train_data_dir = '/content/drive/MyDrive/major/train'\n","validation_data_dir = '/content/drive/MyDrive/major/valid'\n","test_data_dir = '/content/drive/MyDrive/major/test'\n","\n","# Data preprocessing\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","validation_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Load and augment training data\n","train_generator = train_datagen.flow_from_directory(\n","    train_data_dir,\n","    target_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=BATCH_SIZE,\n","    class_mode='binary'\n",")\n","\n","# Load validation data\n","validation_generator = validation_datagen.flow_from_directory(\n","    validation_data_dir,\n","    target_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=BATCH_SIZE,\n","    class_mode='binary'\n",")\n","\n","# Load test data\n","test_generator = test_datagen.flow_from_directory(\n","    test_data_dir,\n","    target_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=BATCH_SIZE,\n","    class_mode=None,  # No labels for test data\n","    shuffle=False  # Do not shuffle test data\n",")\n","\n","# Load pre-trained MobileNetV2 model\n","base_model = MobileNetV2(weights='imagenet', include_top=False)\n","\n","# Add custom classification layers\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(512, activation='relu')(x)\n","predictions = Dense(1, activation='sigmoid')(x)\n","\n","# Combine base model and custom layers\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# Freeze pre-trained layers\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Compile model\n","model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n","    epochs=20,\n","    validation_data=validation_generator,\n","    validation_steps=validation_generator.samples // BATCH_SIZE\n",")\n","\n","\n","# Create a data generator for the test dataset\n","test_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n","\n","test_generator = test_data_generator.flow_from_directory(\n","    test_data_dir,\n","    target_size=(224, 224),  # Adjust according to your model's input size\n","    batch_size=batch_size,\n","    class_mode='binary',  # Change to 'categorical' if your model has multiple classes\n","    shuffle=False  # Important to keep the order of predictions consistent with labels\n",")\n","\n","# Evaluate the model on the test dataset\n","loss, accuracy = model.evaluate(test_generator)\n","\n","print(f\"Test Loss: {loss}\")\n","print(f\"Test Accuracy: {accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wT_8ikjyj7xr","executionInfo":{"status":"ok","timestamp":1712950113842,"user_tz":-330,"elapsed":3705263,"user":{"displayName":"Akanksha Vennu","userId":"16862492790728247499"}},"outputId":"c6e3bf74-05aa-4dab-830b-7012cb4e6bfb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2539 images belonging to 2 classes.\n","Found 100 images belonging to 2 classes.\n","Found 300 images belonging to 2 classes.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","79/79 [==============================] - 171s 2s/step - loss: 0.5901 - accuracy: 0.7104 - val_loss: 0.4937 - val_accuracy: 0.7396\n","Epoch 2/20\n","79/79 [==============================] - 169s 2s/step - loss: 0.5270 - accuracy: 0.7495 - val_loss: 0.4927 - val_accuracy: 0.7083\n","Epoch 3/20\n","79/79 [==============================] - 166s 2s/step - loss: 0.4900 - accuracy: 0.7643 - val_loss: 0.5435 - val_accuracy: 0.6771\n","Epoch 4/20\n","79/79 [==============================] - 166s 2s/step - loss: 0.4755 - accuracy: 0.7682 - val_loss: 0.4237 - val_accuracy: 0.8125\n","Epoch 5/20\n","79/79 [==============================] - 159s 2s/step - loss: 0.4710 - accuracy: 0.7762 - val_loss: 0.4351 - val_accuracy: 0.7396\n","Epoch 6/20\n","79/79 [==============================] - 160s 2s/step - loss: 0.4447 - accuracy: 0.7866 - val_loss: 0.4396 - val_accuracy: 0.7604\n","Epoch 7/20\n","79/79 [==============================] - 165s 2s/step - loss: 0.4467 - accuracy: 0.7942 - val_loss: 0.4611 - val_accuracy: 0.7812\n","Epoch 8/20\n","79/79 [==============================] - 172s 2s/step - loss: 0.4344 - accuracy: 0.7970 - val_loss: 0.4245 - val_accuracy: 0.7604\n","Epoch 9/20\n","79/79 [==============================] - 171s 2s/step - loss: 0.4356 - accuracy: 0.7930 - val_loss: 0.4627 - val_accuracy: 0.8021\n","Epoch 10/20\n","79/79 [==============================] - 165s 2s/step - loss: 0.4340 - accuracy: 0.7982 - val_loss: 0.4445 - val_accuracy: 0.7604\n","Epoch 11/20\n","79/79 [==============================] - 166s 2s/step - loss: 0.4248 - accuracy: 0.8026 - val_loss: 0.4620 - val_accuracy: 0.7604\n","Epoch 12/20\n","79/79 [==============================] - 167s 2s/step - loss: 0.4245 - accuracy: 0.7946 - val_loss: 0.4391 - val_accuracy: 0.8125\n","Epoch 13/20\n","79/79 [==============================] - 162s 2s/step - loss: 0.4238 - accuracy: 0.7970 - val_loss: 0.4679 - val_accuracy: 0.7500\n","Epoch 14/20\n","79/79 [==============================] - 163s 2s/step - loss: 0.4156 - accuracy: 0.7942 - val_loss: 0.4405 - val_accuracy: 0.8021\n","Epoch 15/20\n","79/79 [==============================] - 160s 2s/step - loss: 0.4199 - accuracy: 0.8057 - val_loss: 0.4402 - val_accuracy: 0.7812\n","Epoch 16/20\n","79/79 [==============================] - 168s 2s/step - loss: 0.4079 - accuracy: 0.8053 - val_loss: 0.4489 - val_accuracy: 0.7812\n","Epoch 17/20\n","79/79 [==============================] - 159s 2s/step - loss: 0.4045 - accuracy: 0.8073 - val_loss: 0.4116 - val_accuracy: 0.8021\n","Epoch 18/20\n","79/79 [==============================] - 166s 2s/step - loss: 0.4116 - accuracy: 0.7982 - val_loss: 0.4379 - val_accuracy: 0.7917\n","Epoch 19/20\n","79/79 [==============================] - 161s 2s/step - loss: 0.3862 - accuracy: 0.8257 - val_loss: 0.4391 - val_accuracy: 0.7917\n","Epoch 20/20\n","79/79 [==============================] - 172s 2s/step - loss: 0.3887 - accuracy: 0.8225 - val_loss: 0.4222 - val_accuracy: 0.8125\n","Found 300 images belonging to 2 classes.\n","10/10 [==============================] - 46s 5s/step - loss: 0.4481 - accuracy: 0.8233\n","Test Loss: 0.44805049896240234\n","Test Accuracy: 0.8233333230018616\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOcsDDm2WeHfvc+rvmwRZsi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}